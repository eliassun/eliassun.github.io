#1. Proxmox provider:
# https://registry.terraform.io/providers/bpg/proxmox/latest/docs
provider "proxmox" {
    pm_api_url      = https://proxmox_host/api2/json
    pm_user         = 
    pm_password     = 
    pm_tls_insecure = true (or false)
}

#2. Terraform modules:
terraform {
  required_providers {
    random = {
      source  = "hashicorp/random"
      version = "~> 3.3.0"
    }
    local = {
      source  = "hashicorp/local"
      version = "~> 2.2.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "~> 3.1.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "~> 3.4.0"
    }
  }
  proxmox = {
      source  = "thegameprofi/proxmox" # or telmate/proxmox
      version = ">= 2.9.15"
  }
  required_version = ">= 0.13.7, < 2.0.0"
}


#3. Create a VM from a template
resource "proxmox_vm_qemu" "proxmox_vm" {
    count       = n
    vmid        = random_vm_id
    name        = coalesce(var.proxmox_vm_name, "proxm_vm-${count.index + 1}")
    target_node = # the name of node, e.g. proxmox_node_1

    desc = "my proxmox vm"

    clone                   = # need to make template first
    full_clone              = true
    os_type                 = "cloud-init"
    bios                    = # seabios or ovmf
    scsihw                  = # lsi, lsi53c810, megasas, pvscsi, virtio-scsi-pci, virtio-scsi-single
    qemu_os                 = # "ubuntu" or "other"

    # Cloudinit
    cicustom                = #path to a custom cloud-init config, e.g. user=local:snippets/local_userdata.yml
    cloudinit_cdrom_storage = # storage to create cloudinit,e.g. local-1
    agent                   = 1

    sockets                 = 1
    cores                   = 4
    numa                    = false
    memory                  = 4096

    network {
      model               = # virtio or e1000
      bridge              = # vmbr1...
      firewall            = false
    }


    serial {
        id                  = 0
        type                = "socket"
    }

  lifecycle {
     ignore_changes = [
       network, desc, cloudinit_cdrom_storage, disks
     ]
  }
}

Create image template on the Proxmox host:

qm create 9001 --name qcow2-template --bios seabios --memory 4096 --cpu host --socket 1 --cores 1 --net0 virtio,bridge=vmbr0 --net1 virtio,bridge=vmbr1 --serial0 socket

qm importdisk 9001 /var/lib/vz/template/my_image.qcow2 local-3

qm set 9001 --virtio0 local-3:vm-9001-disk-0

qm set 9001 --boot c --bootdisk virtio0 --scsihw lsi

qm set 9001 --ide2 local-3:cloudinit

qm set 9001 --serial0 socket --vga serial0

qm set 9001 --agent enabled=1

qm template 9001

If not template it, then copy the cloud-init userdata:
copy the local user_data_file_name to the proxmox host /var/lib/vz/{user_data_file_name}
qm set 9001 --cicustom \"user=local:{user_data_file_name}\"
qm start 9001


#4. Copy cloud-init 

resource "null_resource" "copy_cloud_init_config_files" {
    connection {
        type                = "ssh"
        user                = 
        password            = 
        host                = proxmox_host_ip
    }

    provisioner "file" { 
      source      = "local_userdata.yml"
      destination = "/var/lib/vz/snippets/remote_userdata.yml"
    }

  depends_on = [
    local_sensitive_file.local_userdata
  ]
}

#5. Get the IP of the container
resource "null_resource" "get_lxc_ip" {
  provisioner "local-exec" {
    command = <<-EOT
      digit_id=$(basename "${lxc_vm.vmid}")
      ssh username@proxmox_host_ip lxc-info -n $digit_id | awk '/IP:/ {print $2}' > local_file.xt
      lxc_vm_ip=`cat local_file.txt`
      sed -i "s/special_tag/$lxc_vm_ip/g" final_file_need_this_ip
    EOT
  }
  triggers = {
    always_run = timestamp()
  }
}

#6. Audit the VM cd-room and bootdisk on the proxmox host

resource "null_resource" "refresh" {
  triggers = {
    bastion_username   = 
    bastion_password   = 
    bastion_ip         =
    image_template_id  = 
  }
  connection {
    type     = "ssh"
    user     = self.triggers.bastion_username
    password = self.triggers.bastion_password
    host     = self.triggers.bastion_ip
  }

  provisioner "file" {
    content     = <<EOF
#!/bin/bash
attach_cloud_init() {
    qm set ${vm_id} --ide2 local-1:cloudinit
}

attach_boot_disk() {
    qm set ${vm_id}  --virtio0 local-1:vm-${vm_id}-disk-0
}

handle_msg() {
    local err="$1"
    if [ -z "$err" ]; then
        return 0
    elif [[ "$err" == *"already exists"* ]]; then
        return 0 
    else
        return 1
    fi
}

max_execution_time=610 
start_time=$(date + %s)
max_attempts=300
current_attempt=1

while [ $(( $(date +%s) - start_time )) -le $max_execution_time ] && [ $current_attempt -le $max_attempts ]; do
    # Execute command 1 and capture its output and error
    attach_cloud_init
    exit_status=$?
    output1=$(attach_cloud_init 2>&1)
    err_msg="$output1"

    if [ $exit_status -eq 0 ] || handle_msg "$err_msg"; then
        echo "Attached cloud init cd-room. Audit it in 10 seconds"
        sleep 8
        attach_cloud_init
        echo "Ok for attach_cloud_init. Continuing..."
        break  
    else
        echo "attach_cloud_init failed. Retrying..."
        ((current_attempt++))
    fi
    sleep 2
done

current_attempt=1
while [ $(( $(date +%s) - start_time )) -le $max_execution_time ] && [ $current_attempt -le $max_attempts ]; do
    attach_boot_disk
    exit_status=$?
    output2=$(attach_boot_disk 2>&1)
    err_msg="$output2"

    if [ $exit_status -eq 0 ] || handle_msg "$err_msg"; then
        echo "Attached boot disk cd-room. Audit it in 10 seconds"
        sleep 5  
        attach_boot_disk
        echo "Ok for attach_boot_disk.  Continuing ..."
        break  
    else
        echo "attach_boot_disk failed. Retrying..."
        ((current_attempt++))
    fi
    sleep 2
done

EOF
    destination = "file_on_the_remote_server"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x file_on_the_remote_server",
      "file_on_the_remote_server"
    ]
    on_failure = continue
  }
}

#7. Create the container in the Proxmox
resource "proxmox_lxc" "my_container" {
  target_node  = proxmox_node
  hostname     = 
  password     = 
  unprivileged = true
  start        = true
  ssh_public_keys = ssh_public_key
  ostemplate   = "local:vztmpl/ubuntu-22.04-standard_22.04-1_amd64.tar.zst"

  rootfs {
    storage = var.storage
    size    = "8G"
  }

  network {
    name   = "eth0"
    bridge = "vmbr1"
    ip     = "dhcp"
  }
  nameserver =
}

Static IP:
  network {
    name   = "eth0"
    bridge = 
    ip     = 
    gw     = 
  }

#7. get the VM ip 
pvesh get /nodes/<hostname>/qemu/<vmid>/agent/network-get-interfaces

pvesh get /nodes/pve/qemu/201/agent/network-get-interfaces -o json | jq

output=$(qm config ${vmid} | grep net0)
mac_address=$(echo "$output" | awk -F'=' '{print $2}' | cut -d',' -f1)
ip_address=$(arp -n | grep -i "$mac_address" | awk '{print $1}')

